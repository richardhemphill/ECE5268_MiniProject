{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e",
   "display_name": "Python 3.7.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2x Single-Image Super-Resolution on Grayscale Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Assignment:** Individual Class Project<br>\n",
    "**Author:** Richard Hemphill<br>\n",
    "**ID:** 903877709<br>\n",
    "**Class:** ECE5268 Theory of Neural Networks<br>\n",
    "**Instructor:** Dr. Georgios C. Anagnostopoulos<br>\n",
    "**Description:** Using small-sized grayscale images, construct a CNN-based architecture that will downscale (magnify) the images by a factor of 2.<br>\n",
    "**Emphasis:** Describe the concept of single-image super-resolution, describe the architecture in sufficient detail and show indicative training and post-training results.<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**References:**\n",
    "* https://www.kaggle.com/spaceengineer1/alexonly-greyscale\n",
    "* https://www.kaggle.com/c/two-sigma-financial-news/discussion/83593\n",
    "* https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os.path\n",
    "import random\n",
    "import shutil\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RANDRANGE_STOP=10000\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SET_OWNER = 'spaceengineer1'\n",
    "IMAGE_SET_FILE = 'alexonly-greyscale'\n",
    "ZIP_EXTENSION = 'zip'\n",
    "IMAGE_EXTENSION = 'jpg'\n",
    "PROCESSED_IMAGE_FOLDER ='dataSet'\n",
    "TRAIN_FOLDER = 'train'\n",
    "TEST_FOLDER = 'test'\n",
    "RESCALE_FACTOR = 255.0\n",
    "VALIDATION_SPLIT = 0.2\n",
    "CHANNELS = 1\n",
    "ORIG_IMG_SIZE = 64\n",
    "UPSCALE_FACTOR = 2\n",
    "LOW_RES_IMG_SIZE = int(ORIG_IMG_SIZE/UPSCALE_FACTOR)"
   ]
  },
  {
   "source": [
    "## Prepocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw image set\n",
    "def DownloadImageSet(imageSetOwner = IMAGE_SET_OWNER, imageSetFile = IMAGE_SET_FILE):\n",
    "    zipFile = '{}.{}'.format(imageSetFile, ZIP_EXTENSION)\n",
    "    if not os.path.isfile(zipFile):\n",
    "        # connect to the Kaggle Database and download dataset\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_download_files('{}/{}'.format(imageSetOwner, imageSetFile))\n",
    "    # extract the dataset\n",
    "    zf = ZipFile(zipFile)\n",
    "    topDir = ''.join({item.split('/')[0] for item in zf.namelist()})\n",
    "    if not os.path.isdir(topDir):\n",
    "        zf.extractall() \n",
    "        zf.close()\n",
    "\n",
    "    trainDirPre = os.path.join(topDir,TEST_FOLDER)\n",
    "    if os.path.isdir(trainDirPre):\n",
    "        shutil.move(trainDirPre, '.')\n",
    "        \n",
    "    return topDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Images\n",
    "imgFolder = DownloadImageSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageNorm(image):\n",
    "    image = image/RESCALE_FACTOR\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shrink(input):\n",
    "    return tf.image.resize(input,[LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE],method='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 644 files for training.\n"
     ]
    }
   ],
   "source": [
    "trainSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x2C917066DC8>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for batch in trainSet.take(1):\n",
    "    display(array_to_img(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = trainSet.map(ImageNorm)\n",
    "trainSet = trainSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = valSet.map(ImageNorm)\n",
    "valSet = valSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "source": [
    "## Create Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperResolution(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "    convArgs = {\n",
    "        'activation': 'relu',\n",
    "        'kernel_initializer': 'Orthogonal',\n",
    "        'padding': 'same'\n",
    "    }\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=5, **convArgs)(inputs)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, **convArgs)(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=3, **convArgs)(x)\n",
    "    x = keras.layers.Conv2D(filters=(channels * (upscaleFactor ** 2)), kernel_size=3, **convArgs)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscaleFactor)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_21\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_25 (InputLayer)        [(None, None, None, 1)]   0         \n_________________________________________________________________\nconv2d_119 (Conv2D)          (None, None, None, 64)    640       \n_________________________________________________________________\nconv2d_120 (Conv2D)          (None, None, None, 32)    18464     \n_________________________________________________________________\nconv2d_121 (Conv2D)          (None, None, None, 4)     1156      \n_________________________________________________________________\ntf.nn.depth_to_space_23 (TFO (None, None, None, 1)     0         \n=================================================================\nTotal params: 20,260\nTrainable params: 20,260\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sr = SuperResolution()\n",
    "sr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "srOpt = keras.optimizers.SGD(learning_rate=0.001)\n",
    "srLossFn = keras.losses.MeanSquaredError()\n",
    "sr.compile(optimizer=srOpt, loss=srLossFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "65/65 [==============================] - 6s 85ms/step - loss: 0.1745 - val_loss: 0.1548\n",
      "Epoch 2/2\n",
      "65/65 [==============================] - 6s 82ms/step - loss: 0.1521 - val_loss: 0.1387\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c91758cd88>"
      ]
     },
     "metadata": {},
     "execution_count": 629
    }
   ],
   "source": [
    "sr.fit(trainSet, epochs=EPOCHS, validation_data=valSet, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e",
   "display_name": "Python 3.7.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2x Single-Image Super-Resolution on Grayscale Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Assignment:** Individual Class Project<br>\n",
    "**Author:** Richard Hemphill<br>\n",
    "**ID:** 903877709<br>\n",
    "**Class:** ECE5268 Theory of Neural Networks<br>\n",
    "**Instructor:** Dr. Georgios C. Anagnostopoulos<br>\n",
    "**Description:** Using small-sized grayscale images, construct a CNN-based architecture that will downscale (magnify) the images by a factor of 2.<br>\n",
    "**Emphasis:** Describe the concept of single-image super-resolution, describe the architecture in sufficient detail and show indicative training and post-training results.<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**References:**\n",
    "* https://www.kaggle.com/spaceengineer1/alexonly-greyscale\n",
    "* https://www.kaggle.com/c/two-sigma-financial-news/discussion/83593\n",
    "* https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os.path\n",
    "import random\n",
    "import shutil\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RANDRANGE_STOP=10000\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SET_OWNER = 'spaceengineer1'\n",
    "IMAGE_SET_FILE = 'alexonly-greyscale'\n",
    "ZIP_EXTENSION = 'zip'\n",
    "IMAGE_EXTENSION = 'jpg'\n",
    "PROCESSED_IMAGE_FOLDER ='dataSet'\n",
    "TRAIN_FOLDER = 'train'\n",
    "TEST_FOLDER = 'test'\n",
    "RESCALE_FACTOR = 255.0\n",
    "VALIDATION_SPLIT = 0.2\n",
    "CHANNELS = 1\n",
    "ORIG_IMG_SIZE = 64\n",
    "UPSCALE_FACTOR = 2\n",
    "LOW_RES_IMG_SIZE = int(ORIG_IMG_SIZE/UPSCALE_FACTOR)"
   ]
  },
  {
   "source": [
    "## Prepocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw image set\n",
    "def DownloadImageSet(imageSetOwner = IMAGE_SET_OWNER, imageSetFile = IMAGE_SET_FILE):\n",
    "    zipFile = '{}.{}'.format(imageSetFile, ZIP_EXTENSION)\n",
    "    if not os.path.isfile(zipFile):\n",
    "        # connect to the Kaggle Database and download dataset\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_download_files('{}/{}'.format(imageSetOwner, imageSetFile))\n",
    "    # extract the dataset\n",
    "    zf = ZipFile(zipFile)\n",
    "    topDir = ''.join({item.split('/')[0] for item in zf.namelist()})\n",
    "    if not os.path.isdir(topDir):\n",
    "        zf.extractall() \n",
    "        zf.close()\n",
    "\n",
    "    trainDirPre = os.path.join(topDir,TEST_FOLDER)\n",
    "    if os.path.isdir(trainDirPre):\n",
    "        shutil.move(trainDirPre, '.')\n",
    "        \n",
    "    return topDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Images\n",
    "imgFolder = DownloadImageSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageNorm(image):\n",
    "    image = image/RESCALE_FACTOR\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shrink(input):\n",
    "    return tf.image.resize(input,[LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE],method='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 644 files for training.\n"
     ]
    }
   ],
   "source": [
    "trainSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x2C917731C08>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for batch in trainSet.take(1):\n",
    "    display(array_to_img(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = trainSet.map(ImageNorm)\n",
    "trainSet = trainSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = valSet.map(ImageNorm)\n",
    "valSet = valSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "source": [
    "## Create Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperResolution(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=9, activation='relu', padding='same')(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=1, activation='relu', padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=(channels * (upscaleFactor ** 2)), kernel_size=5, activation='relu', padding='same')(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscaleFactor)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_23\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_27 (InputLayer)        [(None, None, None, 1)]   0         \n_________________________________________________________________\nconv2d_125 (Conv2D)          (None, None, None, 64)    5248      \n_________________________________________________________________\nconv2d_126 (Conv2D)          (None, None, None, 32)    2080      \n_________________________________________________________________\nconv2d_127 (Conv2D)          (None, None, None, 4)     3204      \n_________________________________________________________________\ntf.nn.depth_to_space_25 (TFO (None, None, None, 1)     0         \n=================================================================\nTotal params: 10,532\nTrainable params: 10,532\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sr = SuperResolution()\n",
    "sr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "srOpt = keras.optimizers.SGD(learning_rate=0.001)\n",
    "srLossFn = keras.losses.MeanSquaredError()\n",
    "sr.compile(optimizer=srOpt, loss=srLossFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1710 - val_loss: 0.1080\n",
      "Epoch 2/2\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.0858 - val_loss: 0.0563\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c9175932c8>"
      ]
     },
     "metadata": {},
     "execution_count": 659
    }
   ],
   "source": [
    "sr.fit(trainSet, epochs=EPOCHS, validation_data=valSet, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
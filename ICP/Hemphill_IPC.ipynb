{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e",
   "display_name": "Python 3.7.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2x Single-Image Super-Resolution on Grayscale Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Assignment:** Individual Class Project<br>\n",
    "**Author:** Richard Hemphill<br>\n",
    "**ID:** 903877709<br>\n",
    "**Class:** ECE5268 Theory of Neural Networks<br>\n",
    "**Instructor:** Dr. Georgios C. Anagnostopoulos<br>\n",
    "**Description:** Using small-sized grayscale images, construct a CNN-based architecture that will downscale (magnify) the images by a factor of 2.<br>\n",
    "**Emphasis:** Describe the concept of single-image super-resolution, describe the architecture in sufficient detail and show indicative training and post-training results.<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**References:**\n",
    "* https://www.kaggle.com/spaceengineer1/alexonly-greyscale\n",
    "* https://www.kaggle.com/c/two-sigma-financial-news/discussion/83593\n",
    "* https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os.path\n",
    "import random\n",
    "import shutil\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RANDRANGE_STOP=10000\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SET_OWNER = 'spaceengineer1'\n",
    "IMAGE_SET_FILE = 'alexonly-greyscale'\n",
    "ZIP_EXTENSION = 'zip'\n",
    "IMAGE_EXTENSION = 'jpg'\n",
    "PROCESSED_IMAGE_FOLDER ='dataSet'\n",
    "TRAIN_FOLDER = 'train'\n",
    "TEST_FOLDER = 'test'\n",
    "RESCALE_FACTOR = 1./255\n",
    "VALIDATION_SPLIT = 0.2\n",
    "CHANNELS = 1\n",
    "ORIG_IMG_SIZE = 64\n",
    "UPSCALE_FACTOR = 2\n",
    "LOW_RES_IMG_SIZE = int(ORIG_IMG_SIZE/UPSCALE_FACTOR)"
   ]
  },
  {
   "source": [
    "## Prepocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw image set\n",
    "def DownloadImageSet(imageSetOwner = IMAGE_SET_OWNER, imageSetFile = IMAGE_SET_FILE):\n",
    "    zipFile = '{}.{}'.format(imageSetFile, ZIP_EXTENSION)\n",
    "    if not os.path.isfile(zipFile):\n",
    "        # connect to the Kaggle Database and download dataset\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_download_files('{}/{}'.format(imageSetOwner, imageSetFile))\n",
    "    # extract the dataset\n",
    "    zf = ZipFile(zipFile)\n",
    "    topDir = ''.join({item.split('/')[0] for item in zf.namelist()})\n",
    "    if not os.path.isdir(topDir):\n",
    "        zf.extractall() \n",
    "        zf.close()\n",
    "\n",
    "    trainDirPre = os.path.join(topDir,TEST_FOLDER)\n",
    "    if os.path.isdir(trainDirPre):\n",
    "        shutil.move(trainDirPre, '.')\n",
    "        \n",
    "    return topDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Images\n",
    "imgFolder = DownloadImageSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageNorm(image):\n",
    "    image = image/255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shrink(input):\n",
    "    return tf.image.resize(input,[LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE],method='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 644 files for training.\n"
     ]
    }
   ],
   "source": [
    "trainSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x2C912B6ED48>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for batch in trainSet.take(1):\n",
    "    display(array_to_img(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = trainSet.map(ImageNorm)\n",
    "trainSet = trainSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = valSet.map(ImageNorm)\n",
    "valSet = valSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "source": [
    "## Create Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperResolution(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "    convArgs = {\n",
    "        'activation': 'relu',\n",
    "        'kernel_initializer': 'Orthogonal',\n",
    "        'padding': 'same'\n",
    "    }\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=5, **convArgs)(inputs)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, **convArgs)(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=3, **convArgs)(x)\n",
    "    x = keras.layers.Conv2D(filters=(channels * (upscaleFactor ** 2)), kernel_size=3, **convArgs)(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscaleFactor)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_20\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_24 (InputLayer)        [(None, None, None, 1)]   0         \n_________________________________________________________________\nconv2d_115 (Conv2D)          (None, None, None, 64)    640       \n_________________________________________________________________\nconv2d_116 (Conv2D)          (None, None, None, 32)    18464     \n_________________________________________________________________\nconv2d_117 (Conv2D)          (None, None, None, 4)     1156      \n_________________________________________________________________\ntf.nn.depth_to_space_22 (TFO (None, None, None, 1)     0         \n=================================================================\nTotal params: 20,260\nTrainable params: 20,260\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sr = SuperResolution()\n",
    "sr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "srOpt = keras.optimizers.SGD(learning_rate=0.001)\n",
    "srLossFn = keras.losses.MeanSquaredError()\n",
    "sr.compile(optimizer=srOpt, loss=srLossFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "65/65 [==============================] - 6s 86ms/step - loss: 0.2210 - val_loss: 0.2055\n",
      "Epoch 2/2\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.2021 - val_loss: 0.1896\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c9172e4a08>"
      ]
     },
     "metadata": {},
     "execution_count": 614
    }
   ],
   "source": [
    "sr.fit(trainSet, epochs=EPOCHS, validation_data=valSet, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
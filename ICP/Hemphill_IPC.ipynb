{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e",
   "display_name": "Python 3.7.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "e2ee6990e829ee75785e20acf53b05f75aefa7ec77d0c7f557db63932b894e5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 2x Single-Image Super-Resolution on Grayscale Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Assignment:** Individual Class Project<br>\n",
    "**Author:** Richard Hemphill<br>\n",
    "**ID:** 903877709<br>\n",
    "**Class:** ECE5268 Theory of Neural Networks<br>\n",
    "**Instructor:** Dr. Georgios C. Anagnostopoulos<br>\n",
    "**Description:** Using small-sized grayscale images, construct a CNN-based architecture that will downscale (magnify) the images by a factor of 2.<br>\n",
    "**Emphasis:** Describe the concept of single-image super-resolution, describe the architecture in sufficient detail and show indicative training and post-training results.<br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**References:**\n",
    "* https://www.kaggle.com/spaceengineer1/alexonly-greyscale\n",
    "* https://www.kaggle.com/c/two-sigma-financial-news/discussion/83593\n",
    "* https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os.path\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import PIL\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CURRENT_DIRECTORY = '.'\n",
    "RANDRANGE_STOP = 10000\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SET_OWNER = 'spaceengineer1'\n",
    "IMAGE_SET_FILE = 'alexonly-greyscale'\n",
    "ZIP_EXTENSION = 'zip'\n",
    "IMAGE_EXTENSION = 'jpg'\n",
    "PROCESSED_IMAGE_FOLDER ='dataSet'\n",
    "TRAIN_FOLDER = 'train'\n",
    "TEST_FOLDER = 'test'\n",
    "RESCALE_FACTOR = 255.0\n",
    "VALIDATION_SPLIT = 0.2\n",
    "CHANNELS = 1\n",
    "ORIG_IMG_SIZE = 64\n",
    "UPSCALE_FACTOR = 2\n",
    "LOW_RES_IMG_SIZE = int(ORIG_IMG_SIZE/UPSCALE_FACTOR)"
   ]
  },
  {
   "source": [
    "## Prepocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw image set\n",
    "def DownloadImageSet(imageSetOwner = IMAGE_SET_OWNER, imageSetFile = IMAGE_SET_FILE):\n",
    "    zipFile = '{}.{}'.format(imageSetFile, ZIP_EXTENSION)\n",
    "    if not os.path.isfile(zipFile):\n",
    "        # connect to the Kaggle Database and download dataset\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        api.dataset_download_files('{}/{}'.format(imageSetOwner, imageSetFile))\n",
    "    # extract the dataset\n",
    "    zf = ZipFile(zipFile)\n",
    "    topDir = ''.join({item.split('/')[0] for item in zf.namelist()})\n",
    "    if not os.path.isdir(topDir):\n",
    "        zf.extractall() \n",
    "        zf.close()\n",
    "\n",
    "    trainDirPre = os.path.join(topDir,TEST_FOLDER)\n",
    "    if os.path.isdir(trainDirPre):\n",
    "        shutil.move(trainDirPre, CURRENT_DIRECTORY)\n",
    "        \n",
    "    return topDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Images\n",
    "imgFolder = DownloadImageSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageNorm(image):\n",
    "    image = image/RESCALE_FACTOR\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shrink(input):\n",
    "    return tf.image.resize(input,[LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE],method='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 644 files for training.\n"
     ]
    }
   ],
   "source": [
    "trainSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x2C947D39408>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "for batch in trainSet.take(1):\n",
    "    display(array_to_img(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = trainSet.map(ImageNorm)\n",
    "trainSet = trainSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 804 files belonging to 1 classes.\nUsing 160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valSet = image_dataset_from_directory(\n",
    "    directory=imgFolder,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(ORIG_IMG_SIZE,ORIG_IMG_SIZE),\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    seed=random.randrange(RANDRANGE_STOP),\n",
    "    label_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "valSet = valSet.map(ImageNorm)\n",
    "valSet = valSet.map(lambda x: (Shrink(x),x))"
   ]
  },
  {
   "source": [
    "## Create Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuperResolution(upscaleFactor=UPSCALE_FACTOR, channels=CHANNELS):\n",
    "\n",
    "    inputs = keras.Input(shape=(None, None, channels))\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=9, activation='relu', padding='same')(inputs)\n",
    "    x = keras.layers.Conv2D(filters=32, kernel_size=1, activation='relu', padding='same')(x)\n",
    "    x = keras.layers.Conv2D(filters=(channels * (upscaleFactor ** 2)), kernel_size=5, activation='relu', padding='same')(x)\n",
    "    outputs = tf.nn.depth_to_space(x, upscaleFactor)\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_37\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_41 (InputLayer)        [(None, None, None, 1)]   0         \n_________________________________________________________________\nconv2d_167 (Conv2D)          (None, None, None, 64)    5248      \n_________________________________________________________________\nconv2d_168 (Conv2D)          (None, None, None, 32)    2080      \n_________________________________________________________________\nconv2d_169 (Conv2D)          (None, None, None, 4)     3204      \n_________________________________________________________________\ntf.nn.depth_to_space_39 (TFO (None, None, None, 1)     0         \n=================================================================\nTotal params: 10,532\nTrainable params: 10,532\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sr = SuperResolution()\n",
    "sr.summary()"
   ]
  },
  {
   "source": [
    "## Train the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "srOpt = keras.optimizers.SGD(learning_rate=0.001)\n",
    "srLossFn = keras.losses.MeanSquaredError()\n",
    "sr.compile(optimizer=srOpt, loss=srLossFn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "65/65 [==============================] - 6s 86ms/step - loss: 0.1967 - val_loss: 0.1755\n",
      "Epoch 2/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1649 - val_loss: 0.1484\n",
      "Epoch 3/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1401 - val_loss: 0.1325\n",
      "Epoch 4/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1272 - val_loss: 0.1261\n",
      "Epoch 5/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1226 - val_loss: 0.1242\n",
      "Epoch 6/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1214 - val_loss: 0.1237\n",
      "Epoch 7/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1221 - val_loss: 0.1235\n",
      "Epoch 8/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1221 - val_loss: 0.1234\n",
      "Epoch 9/20\n",
      "65/65 [==============================] - 6s 83ms/step - loss: 0.1214 - val_loss: 0.1233\n",
      "Epoch 10/20\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.1209 - val_loss: 0.1232\n",
      "Epoch 11/20\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.1199 - val_loss: 0.1231\n",
      "Epoch 12/20\n",
      "65/65 [==============================] - 6s 85ms/step - loss: 0.1222 - val_loss: 0.1230\n",
      "Epoch 13/20\n",
      "65/65 [==============================] - 6s 87ms/step - loss: 0.1202 - val_loss: 0.1229\n",
      "Epoch 14/20\n",
      "65/65 [==============================] - 6s 88ms/step - loss: 0.1207 - val_loss: 0.1228\n",
      "Epoch 15/20\n",
      "65/65 [==============================] - 6s 86ms/step - loss: 0.1208 - val_loss: 0.1227\n",
      "Epoch 16/20\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.1210 - val_loss: 0.1226\n",
      "Epoch 17/20\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.1206 - val_loss: 0.1226\n",
      "Epoch 18/20\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.1218 - val_loss: 0.1225\n",
      "Epoch 19/20\n",
      "65/65 [==============================] - 6s 84ms/step - loss: 0.1200 - val_loss: 0.1224\n",
      "Epoch 20/20\n",
      "65/65 [==============================] - 6s 86ms/step - loss: 0.1206 - val_loss: 0.1223\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c94313a808>"
      ]
     },
     "metadata": {},
     "execution_count": 974
    }
   ],
   "source": [
    "sr.fit(trainSet, epochs=EPOCHS, validation_data=valSet, use_multiprocessing=True, verbose=1)"
   ]
  },
  {
   "source": [
    "## Test the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MagnifyImage(model, lowResImg):\n",
    "    lowResArr = img_to_array(lowResImg)\n",
    "    lowResArr = lowResArr.astype('float32')/RESCALE_FACTOR\n",
    "    input = np.expand_dims(lowResArr, axis=0)\n",
    "    out = model.predict(input)\n",
    "    out *= RESCALE_FACTOR\n",
    "    out = out.reshape((ORIG_IMG_SIZE,ORIG_IMG_SIZE,CHANNELS))\n",
    "    out = array_to_img(x=out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, testImgPath in enumerate(testImgPaths):\n",
    "    if idx > 0:\n",
    "        continue\n",
    "    img = load_img(testImgPath, color_mode='grayscale')\n",
    "    lowResImg = img.resize((LOW_RES_IMG_SIZE,LOW_RES_IMG_SIZE), resample=PIL.Image.BICUBIC)\n",
    "    superResImg = MagnifyImage(sr,lowResImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=32x32 at 0x2C94319BCC8>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAADFklEQVR4nCXSSY4cVRAG4D/ixcvMyszqcrXdgxsbg8wggTASC8SOLUfgXJyAG7DnCoAAIWw1tsUgTz3VkHO+IR4LX+BbffTd7PHUv//Rl2StzWoB8P3v787JSNSUdlf8D6GuXLMkImZDAECci/XTECezfiC2MCujY2PAbIQBICmrZzJk3Qj5r8jmireGwWxzAgAYOzpJ7cGUkeXiVXjcjJMzbEzGAABOo8m0s93k0kt5YMr1y+Iks8jEAACQkgmxbSVwH15J2eyLo8vNiyd5/uFbQB0ld9PYpBryWv7cVXPhy/Ouq779FADw5lLbvtuvgqbpuOBdqM5ecKyZ4i8KAIje9FfxYOk5unSb59KtTv1mS6S7614BJGgdijQVkMIcyuZ+/kedjhZTUJ32d2v4n7o4h2ZW6bPF/kLmjoq/V3vuRm5aQ8DN84kudNgdlYHkvJCsRXHSrDQOuNh9vABeXbWc93M9N0uZ2lZ4fdkd2SmNwU5Pz/QYs+d5o5XEA7cxZOU4UkPLGz4zNpzjG6Brh6LGcJXFfrzHlVw0KYTuTqwGH67So1NMm1zjdN1WJityAxnvvRzz8q9Sza153/x4OvxWhNcU8yKUHIx4CcM7sy9ltRls4PT4h/KFdHGZXx3My/0a+0ei27Tu2urh/Ia5H9c/x9xmblNl7XtuN5VffS5ZsZfPzlbyvJiKLLfB71x+O+iC3rzefvL1/lc6qQ8OrVmUIxUGRWnMs/4WpajQvpkPu3/pi3qdnpnp1tEdp6XJsZicd4HhImF47ZzcD8MInN6VbVY4a5JRBdSZQietPgizbBLnD0vb75bZbGxA0KRs4H25DN5LJmFRZRL2cc0ORMrx7WqOjS1L54McZ+xvxuowzDHXCcwcQkzqg477+iBLYtNw099ZjiESiSQ2KSGpGngTmrHMZeg7OiknTUCipKQIXQPDlVHWebKyCflxcd1GMIE5q6jb9skjZcZYYZnleoEtXw4xscuRMg1BYYOJDhRJFguqE4hZo1rPHJVASAlIiQgMEYFhJjJMRCACAUCKGmOMMapL/wPWmNrjfOQIwgAAAABJRU5ErkJggg==\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(lowResImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=64x64 at 0x2C94313AF88>",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "display(superResImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}